{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd4f5c02-66c1-4937-9fed-6ce3317f9dca",
   "metadata": {},
   "source": [
    " # Steering Demo\n",
    "\n",
    " Apply control vectors to drum latents and decode to audio.\n",
    "\n",
    " Prerequisites:\n",
    " - Trained SAE checkpoint\n",
    " - feature_summary.csv from evaluation notebook\n",
    " - Stable Audio Open VAE (for decoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89610de7-6a29-4335-8cf1-4055fb932f86",
   "metadata": {},
   "source": [
    " # Steering Demo\n",
    "\n",
    " Apply control vectors to drum latents and decode to audio.\n",
    "\n",
    " Prerequisites:\n",
    " - Trained SAE checkpoint\n",
    " - feature_summary.csv from evaluation notebook\n",
    " - Stable Audio Open VAE (for decoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2695ecbb-44b6-48f7-8a9e-364ad725d642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ[\"DYLD_FALLBACK_LIBRARY_PATH\"] = \"/usr/local/ffmpeg7/lib\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "try:\n",
    "    # If running as regular .py script, __file__ is defined\n",
    "    PROJECT_ROOT = Path(__file__).resolve().parent.parent\n",
    "except NameError:\n",
    "    PROJECT_ROOT = Path.cwd()\n",
    "    if PROJECT_ROOT.name == \"notebooks\":\n",
    "        PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "\n",
    "sys.path.insert(0, str(PROJECT_ROOT / \"src\"))\n",
    "\n",
    "from drums_SAE.sae.model import AudioSae\n",
    "from drums_SAE.steering.steer import ControlVectors, create_steering_grid, steer_latent\n",
    "from drums_SAE.training.data import LatentDataset\n",
    "\n",
    "DEVICE = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688e9b25-432a-4fef-9595-32605bf0247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = PROJECT_ROOT / \"checkpoints/sae_step_50000.pt\"\n",
    "FEATURE_SUMMARY_PATH = PROJECT_ROOT / \"notebooks/feature_summary.csv\"\n",
    "LATENT_DATA_PATH = PROJECT_ROOT / \"data/drums_encoded.npz\"\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"outputs/steering\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SAMPLE_RATE = 44100  # Stable Audio Open sample rate\n",
    "N_TIMESTEPS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3946226f-b784-4044-be1c-b0512df0d81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SAE and control vectors...\n",
      "Control vectors available for: ['brightness', 'boominess', 'warmth', 'hardness', 'depth', 'roughness', 'sharpness', 'loudness', 'reverb']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AudioSae(\n",
       "  (encoder): Linear(in_features=64, out_features=1024, bias=False)\n",
       "  (decoder): Linear(in_features=1024, out_features=64, bias=False)\n",
       "  (rms_norm): RMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading SAE and control vectors...\")\n",
    "cv = ControlVectors.from_checkpoint(\n",
    "    str(CHECKPOINT_PATH),\n",
    "    str(FEATURE_SUMMARY_PATH),\n",
    "    top_k=20,\n",
    "    device=DEVICE,\n",
    ")\n",
    "print(f\"Control vectors available for: {list(cv.keys())}\")\n",
    "\n",
    "# Also load the full SAE for encoding/decoding through SAE\n",
    "checkpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE, weights_only=False)\n",
    "cfg = checkpoint[\"config\"]\n",
    "sae = AudioSae(\n",
    "    d_input=cfg[\"d_input\"],\n",
    "    expansion_factor=cfg[\"expansion_factor\"],\n",
    "    topk=cfg[\"topk\"],\n",
    "    topk_aux=cfg[\"topk_aux\"],\n",
    "    dead_threshold=cfg[\"dead_threshold\"],\n",
    ").to(DEVICE)\n",
    "sae.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "sae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528a70ee-71f3-4b16-b7fc-11ff6a63c994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 328,128 latent vecs from /Users/omarhammami/hambaLab/Output/drums_SAE/data/drums_encoded.npz\n",
      "Dataset: 328,128 latent vectors\n",
      "Audio samples: 10,254\n"
     ]
    }
   ],
   "source": [
    "dataset = LatentDataset(str(LATENT_DATA_PATH), normalize=True)\n",
    "\n",
    "# Load normalization stats for denormalization\n",
    "latent_data = np.load(LATENT_DATA_PATH)\n",
    "latent_mean = torch.tensor(latent_data[\"mean\"], dtype=torch.float32)\n",
    "latent_std = torch.tensor(latent_data[\"std\"], dtype=torch.float32)\n",
    "\n",
    "print(f\"Dataset: {len(dataset):,} latent vectors\")\n",
    "print(f\"Audio samples: {len(dataset) // N_TIMESTEPS:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa2f794-3bf9-4daf-926f-3b75a495916b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Stable Audio Open VAE...\n",
      "No module named 'flash_attn'\n",
      "flash_attn not installed, disabling Flash Attention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/omarhammami/hambaLab/Output/drums_SAE/.venv/lib/python3.12/site-packages/clip/clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import packaging\n",
      "/Users/omarhammami/hambaLab/Output/drums_SAE/.venv/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading Stable Audio Open VAE...\")\n",
    "\n",
    "try:\n",
    "    from einops import rearrange\n",
    "    from stable_audio_tools import get_pretrained_model\n",
    "    from stable_audio_tools.models.utils import load_ckpt_state_dict\n",
    "\n",
    "    # Load the pretrained model (this will download if needed)\n",
    "    model, model_config = get_pretrained_model(\"stabilityai/stable-audio-open-1.0\")\n",
    "    vae = model.pretransform.model  # The autoencoder\n",
    "    vae = vae.to(DEVICE)\n",
    "    vae.eval()\n",
    "\n",
    "    VAE_AVAILABLE = True\n",
    "    print(\"‚úì VAE loaded successfully\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö† Could not load VAE: {e}\")\n",
    "    print(\"Install with: pip install stable-audio-tools\")\n",
    "    print(\"Steering will work but you won't be able to decode to audio.\")\n",
    "    VAE_AVAILABLE = False\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Error loading VAE: {e}\")\n",
    "    VAE_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0893bcc0-97f0-45ed-b8a9-82f836184b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_latent(z_norm: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Convert normalized latent back to original scale.\"\"\"\n",
    "    return z_norm * latent_std + latent_mean\n",
    "\n",
    "\n",
    "def decode_to_audio(z_norm: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Decode normalized latent(s) to audio.\n",
    "\n",
    "    Args:\n",
    "        z_norm: Normalized latent, shape (32, 64) for one audio sample\n",
    "                or (batch, 32, 64) for multiple\n",
    "\n",
    "    Returns:\n",
    "        Audio waveform, shape (samples,) or (batch, samples)\n",
    "    \"\"\"\n",
    "    if not VAE_AVAILABLE:\n",
    "        raise RuntimeError(\"VAE not available. Cannot decode to audio.\")\n",
    "\n",
    "    # Ensure 3D: (batch, timesteps, channels)\n",
    "    if z_norm.dim() == 2:\n",
    "        z_norm = z_norm.unsqueeze(0)\n",
    "\n",
    "    # Denormalize\n",
    "    z = denormalize_latent(z_norm.cpu())\n",
    "\n",
    "    # VAE expects (batch, channels, timesteps)\n",
    "    z = rearrange(z, \"b t c -> b c t\").to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        audio = vae.decode(z)\n",
    "\n",
    "    # Output is (batch, 1, samples) or (batch, 2, samples)\n",
    "    audio = audio.squeeze(1)  # Remove channel dim if mono\n",
    "\n",
    "    if audio.shape[0] == 1:\n",
    "        audio = audio.squeeze(0)\n",
    "\n",
    "    return audio.cpu()\n",
    "\n",
    "\n",
    "def save_audio(audio: torch.Tensor, path: str, sample_rate: int = SAMPLE_RATE):\n",
    "    \"\"\"Save audio tensor to file.\"\"\"\n",
    "    if audio.dim() == 1:\n",
    "        audio = audio.unsqueeze(0)\n",
    "    torchaudio.save(path, audio, sample_rate)\n",
    "    print(f\"Saved: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc51fc1d-4b31-4aeb-851c-08549fb3e636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_latents(audio_idx: int) -> torch.Tensor:\n",
    "    \"\"\"Get all 32 latent vectors for one audio sample.\"\"\"\n",
    "    start = audio_idx * N_TIMESTEPS\n",
    "    end = start + N_TIMESTEPS\n",
    "    return torch.stack([dataset[i] for i in range(start, end)])  # (32, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c6351c-35c6-44be-81c3-36efb80f35d9",
   "metadata": {},
   "source": [
    " ## Steering Experiments\n",
    "\n",
    " Let's steer some drums!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb153f9-66e1-4331-9a72-19897ab5f68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_IDX = 42  # Change this to try different drums\n",
    "\n",
    "z_original = get_audio_latents(AUDIO_IDX)\n",
    "print(f\"Original latent shape: {z_original.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0619281-4c5b-4ca7-988a-c04888c1b1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = \"brightness\"\n",
    "ALPHAS = [-1.0, -0.5, 0, 0.5, 1.0]\n",
    "\n",
    "direction = cv[LABEL].unsqueeze(0)  # (1, 64) for broadcasting\n",
    "\n",
    "steered_latents = {}\n",
    "for alpha in ALPHAS:\n",
    "    key = f\"{LABEL}_{alpha:+.1f}\"\n",
    "    # Steer each timestep\n",
    "    z_steered = steer_latent(z_original, direction, alpha)\n",
    "    steered_latents[key] = z_steered\n",
    "    print(f\"{key}: mean diff = {(z_steered - z_original).abs().mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753877e9-348e-4eb8-a0a6-bf42d7e2bf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if VAE_AVAILABLE:\n",
    "    print(\"\\nDecoding steered latents to audio...\")\n",
    "\n",
    "    for key, z in steered_latents.items():\n",
    "        audio = decode_to_audio(z)\n",
    "        save_audio(audio, str(OUTPUT_DIR / f\"audio_{AUDIO_IDX}_{key}.wav\"))\n",
    "\n",
    "    # Also save original\n",
    "    audio_orig = decode_to_audio(z_original)\n",
    "    save_audio(audio_orig, str(OUTPUT_DIR / f\"audio_{AUDIO_IDX}_original.wav\"))\n",
    "\n",
    "    print(f\"\\n‚úì Audio files saved to {OUTPUT_DIR}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9465dba-2a62-4929-9817-874dce263f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if VAE_AVAILABLE:\n",
    "    print(f\"\\nüéß Listening comparison for '{LABEL}':\\n\")\n",
    "\n",
    "    for alpha in ALPHAS:\n",
    "        if alpha == 0:\n",
    "            label = \"original\"\n",
    "            z = z_original\n",
    "        else:\n",
    "            label = f\"{LABEL} Œ±={alpha:+.1f}\"\n",
    "            z = steered_latents[f\"{LABEL}_{alpha:+.1f}\"]\n",
    "\n",
    "        audio = decode_to_audio(z)\n",
    "        print(f\"{label}:\")\n",
    "        display(Audio(audio.numpy(), rate=SAMPLE_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd30224-7818-47b0-9ea0-7a320d357fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if VAE_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"MULTI-PROPERTY STEERING GRID\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    LABELS_TO_TEST = [\"brightness\", \"depth\", \"loudness\", \"roughness\"]\n",
    "    ALPHA = 0.7  # Steering strength\n",
    "\n",
    "    for label in LABELS_TO_TEST:\n",
    "        if label not in cv:\n",
    "            continue\n",
    "\n",
    "        direction = cv[label].unsqueeze(0)\n",
    "\n",
    "        z_plus = steer_latent(z_original, direction, alpha=ALPHA)\n",
    "        z_minus = steer_latent(z_original, direction, alpha=-ALPHA)\n",
    "\n",
    "        audio_plus = decode_to_audio(z_plus)\n",
    "        audio_minus = decode_to_audio(z_minus)\n",
    "\n",
    "        save_audio(audio_plus, str(OUTPUT_DIR / f\"audio_{AUDIO_IDX}_{label}_plus.wav\"))\n",
    "        save_audio(\n",
    "            audio_minus, str(OUTPUT_DIR / f\"audio_{AUDIO_IDX}_{label}_minus.wav\")\n",
    "        )\n",
    "\n",
    "        print(f\"\\n{label.upper()}:\")\n",
    "        print(f\"  Less {label} (Œ±=-{ALPHA}):\")\n",
    "        display(Audio(audio_minus.numpy(), rate=SAMPLE_RATE))\n",
    "        print(f\"  More {label} (Œ±=+{ALPHA}):\")\n",
    "        display(Audio(audio_plus.numpy(), rate=SAMPLE_RATE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3c6f62-f50b-49e5-be68-59dc017907e6",
   "metadata": {},
   "source": [
    " ## Steering Through SAE\n",
    "\n",
    " Alternative: Steer in SAE feature space instead of latent space.\n",
    " This is more \"principled\" since we're manipulating the interpretable features directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5f52a3-3b56-4ff0-abd5-8da2d0b1313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def steer_through_sae(\n",
    "    z: torch.Tensor,\n",
    "    sae: AudioSae,\n",
    "    feature_idx: int,\n",
    "    delta: float,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Steer by directly modifying a SAE feature's activation.\n",
    "\n",
    "    Args:\n",
    "        z: Latent(s), shape (timesteps, d_input)\n",
    "        sae: Trained SAE model\n",
    "        feature_idx: Which feature to modify\n",
    "        delta: Amount to add to the feature activation\n",
    "\n",
    "    Returns:\n",
    "        Steered latent(s)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Encode to SAE features\n",
    "        z_device = z.to(DEVICE)\n",
    "        out = sae.encode(z_device)\n",
    "        h = out[\"h\"]  # (timesteps, d_hidden)\n",
    "\n",
    "        # Modify specific feature\n",
    "        h_steered = h.clone()\n",
    "        h_steered[:, feature_idx] = h_steered[:, feature_idx] + delta\n",
    "\n",
    "        # Decode back\n",
    "        # Note: We use h directly (pre-RMSNorm) for decoding\n",
    "        # Need to apply RMSNorm first\n",
    "        h_norm = h_steered.pow(2).mean(dim=-1, keepdim=True).sqrt()\n",
    "        f_steered = h_steered / (h_norm + 1e-8)\n",
    "\n",
    "        z_steered = sae.decode(f_steered)\n",
    "\n",
    "    return z_steered.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651faf80-ad5c-4ff2-be52-9d94f5a49dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find feature most correlated with brightness\n",
    "import pandas as pd\n",
    "\n",
    "feature_summary = pd.read_csv(FEATURE_SUMMARY_PATH)\n",
    "brightness_corrs = feature_summary[\"corr_brightness\"].values\n",
    "best_brightness_feature = np.argmax(brightness_corrs)\n",
    "print(\n",
    "    f\"Best brightness feature: F{best_brightness_feature} (œÅ = {brightness_corrs[best_brightness_feature]:.3f})\"\n",
    ")\n",
    "\n",
    "# Steer using that feature\n",
    "z_feat_plus = steer_through_sae(z_original, sae, best_brightness_feature, delta=2.0)\n",
    "z_feat_minus = steer_through_sae(z_original, sae, best_brightness_feature, delta=-2.0)\n",
    "\n",
    "if VAE_AVAILABLE:\n",
    "    print(\"\\nSteering via SAE feature manipulation:\")\n",
    "\n",
    "    audio_feat_plus = decode_to_audio(z_feat_plus)\n",
    "    audio_feat_minus = decode_to_audio(z_feat_minus)\n",
    "\n",
    "    save_audio(\n",
    "        audio_feat_plus,\n",
    "        str(\n",
    "            OUTPUT_DIR / f\"audio_{AUDIO_IDX}_feature{best_brightness_feature}_plus.wav\"\n",
    "        ),\n",
    "    )\n",
    "    save_audio(\n",
    "        audio_feat_minus,\n",
    "        str(\n",
    "            OUTPUT_DIR / f\"audio_{AUDIO_IDX}_feature{best_brightness_feature}_minus.wav\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    print(f\"\\nF{best_brightness_feature} (brightness) manipulation:\")\n",
    "    print(\"  Decreased:\")\n",
    "    display(Audio(audio_feat_minus.numpy(), rate=SAMPLE_RATE))\n",
    "    print(\"  Increased:\")\n",
    "    display(Audio(audio_feat_plus.numpy(), rate=SAMPLE_RATE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f87ae7-126d-4d41-ad0d-a466e7febe91",
   "metadata": {},
   "source": [
    " ## Batch Generation\n",
    "\n",
    " Generate a batch of steered samples for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15b175f-f99f-499b-85f9-5b27c9a91d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "if VAE_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"BATCH GENERATION\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    N_SAMPLES = 5\n",
    "    LABELS = [\"brightness\", \"depth\", \"roughness\"]\n",
    "    ALPHA = 0.6\n",
    "\n",
    "    sample_indices = np.random.choice(\n",
    "        len(dataset) // N_TIMESTEPS, N_SAMPLES, replace=False\n",
    "    )\n",
    "\n",
    "    batch_dir = OUTPUT_DIR / \"batch\"\n",
    "    batch_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    for idx in sample_indices:\n",
    "        z = get_audio_latents(idx)\n",
    "\n",
    "        # Original\n",
    "        audio = decode_to_audio(z)\n",
    "        save_audio(audio, str(batch_dir / f\"sample_{idx:04d}_original.wav\"))\n",
    "\n",
    "        # Steered versions\n",
    "        for label in LABELS:\n",
    "            if label not in cv:\n",
    "                continue\n",
    "            direction = cv[label].unsqueeze(0)\n",
    "\n",
    "            z_plus = steer_latent(z, direction, ALPHA)\n",
    "            z_minus = steer_latent(z, direction, -ALPHA)\n",
    "\n",
    "            audio_plus = decode_to_audio(z_plus)\n",
    "            audio_minus = decode_to_audio(z_minus)\n",
    "\n",
    "            save_audio(\n",
    "                audio_plus, str(batch_dir / f\"sample_{idx:04d}_{label}_plus.wav\")\n",
    "            )\n",
    "            save_audio(\n",
    "                audio_minus, str(batch_dir / f\"sample_{idx:04d}_{label}_minus.wav\")\n",
    "            )\n",
    "\n",
    "    print(\n",
    "        f\"\\n‚úì Generated {N_SAMPLES * (1 + len(LABELS) * 2)} audio files in {batch_dir}/\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eaf2d9-4d79-4d29-8919-39d62419c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STEERING DEMO COMPLETE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\"\"\n",
    "Files generated in: {OUTPUT_DIR}/\n",
    "\n",
    "Two steering methods demonstrated:\n",
    "1. Control vector steering (add direction to latent)\n",
    "2. SAE feature steering (modify feature activation directly)\n",
    "\n",
    "Next steps:\n",
    "- Listen to outputs and evaluate quality\n",
    "- Tune alpha values for best results\n",
    "- Build interactive demo (Gradio/Streamlit)\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
